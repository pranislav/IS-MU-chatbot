from llama_index.core import VectorStoreIndex, Document, StorageContext, load_index_from_storage
from llama_index.llms.huggingface import HuggingFaceLLM
import json
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.prompts import PromptTemplate
import torch


class E5Embedding(HuggingFaceEmbedding):
    def _get_query_embedding(self, query: str):
        return super()._get_query_embedding(f"query: {query}")

    def _get_text_embedding(self, text: str):
        return super()._get_text_embedding(f"passage: {text}")


def load_or_create_index(PERSIST_DIR):
    embed_model = E5Embedding(model_name="intfloat/multilingual-e5-base")
    if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
        print("üîÑ Loading existing index...")
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        index = load_index_from_storage(storage_context, embed_model=embed_model)
    else:
        print("‚ú® Creating new index...")
        with open("dataset/transformed_for_llamaindex.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            documents = [Document(text=block["text"], metadata=block.get("metadata", {})) for block in data]
        index = VectorStoreIndex.from_documents(
            documents,
            embed_model=embed_model,
        )
        index.storage_context.persist(persist_dir=PERSIST_DIR)
        print("index created")
    return index


def format_prompt(query, context_str, tokenizer):
    system_msg = '''Jsi n√°pomocn√Ω chatbot Masarykovy univerzity. Tv√Ωm √∫kolem je pom√°hat u≈æivatel≈Øm orientovat se v Informaƒçn√≠m syst√©mu (IS MU) a poskytovat rady, jak prov√©st po≈æadovan√© akce v syst√©mu.
    N√≠≈æe jsou ofici√°ln√≠ dokumenty n√°povƒõdy IS MU, kter√© mohou obsahovat u≈æiteƒçn√© informace. Pokud informace ve zdroj√≠ch nejsou dostateƒçn√©, ≈ôekni to up≈ô√≠mnƒõ.
    Zaƒç√°tek nalezen√Ωch dokument≈Ø:
    {context_str}
    Konec nalezen√Ωch dokument≈Ø.
    Moje ot√°zka:'''
    
    messages = [
        {"role": "system", "content": system_msg.format(context_str=context_str)},
        {"role": "user", "content": query}
    ]
    
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def query_augment_prompt(query, tokenizer):
    prompt = "Vygeneruj 3 r≈Øzn√© varianty n√°sleduj√≠c√≠ho dotazu, kter√© maj√≠ stejn√Ω v√Ωznam, ale jinou formulaci. V√Ωsledek vra≈• v√Ωhradnƒõ jako JSON seznam ≈ôetƒõzc≈Ø, bez jak√©hokoliv form√°tov√°n√≠ k√≥du (nepou≈æ√≠vej ``` ani ≈æ√°dn√© znaƒçky).\nDotaz: {query}"
    messages = [
        {"role": "user", "content": prompt.format(query=query)},
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def augment_query(query, tokenizer, pipeline):
    augmented_query = pipeline(query_augment_prompt(query, tokenizer), max_new_tokens=400, do_sample=True, return_full_text=False)[0]["generated_text"]
    augmented_query_list = json.loads(augmented_query)
    return augmented_query_list


def retrieve_documents(index, list_of_queries):
    unique_retrieved_docs_ids = set()
    unique_retrieverd_nodes = []
    retriever = index.as_retriever(similarity_top_k=3)
    for query in list_of_queries:
        retrieved_nodes = retriever.retrieve(query)
        for node in retrieved_nodes:
            if node.metadata["id"] not in unique_retrieved_docs_ids:
                unique_retrieved_docs_ids.add(node.metadata["id"])
                unique_retrieverd_nodes.append(node)
    return unique_retrieverd_nodes


def query_is_muni(query, index, tokenizer, pipeline):
    list_of_queries = augment_query(query, tokenizer, pipeline)
    retrieved_nodes = retrieve_documents(index, list_of_queries)
    context_str = "\n\n".join([n.node.get_content() for n in retrieved_nodes])
    formatted_prompt = format_prompt(query, context_str, tokenizer)
    response = pipeline(formatted_prompt, max_new_tokens=1024, do_sample=True, return_full_text=False)[0]["generated_text"]
    return response


def main():
    PERSIST_DIR = "./dataset/index"
    model_name = "google/gemma-3-4b-it"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    my_pipeline = pipeline("text-generation", model=model_name)
    index = load_or_create_index(PERSIST_DIR)

    while True:
        query = input("Zadejte dotaz nebo 'q' pro ukonƒçen√≠: ")
        if query.lower() == 'q':
            break
        response = query_is_muni(query, index, tokenizer, my_pipeline)
        print(response)


if __name__ == "__main__":
    main()
